{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nODwmF0MEOQb"
      },
      "source": [
        "# Instruction Finetuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BGrlf29Eowf"
      },
      "source": [
        "### 데이터셋 구축\n",
        "\n",
        "1. 목적 정의: 먼저, 세부 튜닝을 통해 달성하고자 하는 목표를 명확히 합니다.\n",
        "1. 데이터 수집: 목표에 맞는 데이터를 수집합니다. 이 데이터는 공개 데이터셋일 수도 있고, 사용자가 직접 수집한 데이터일 수도 있습니다.\n",
        "\n",
        "1. 데이터 가공: 수집한 데이터를 모델 훈련에 적합하게 가공합니다. 이 과정에서는 데이터를 정제하고, 필요한 형식으로 변환하는 작업이 포함됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9r0smmEIAjT"
      },
      "source": [
        "### 공개 데이터셋 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ilVIo8M0IE1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets==2.17.0\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (3.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (0.70.16)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0)\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (0.23.4)\n",
            "Requirement already satisfied: packaging in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from datasets==2.17.0) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.17.0) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from pandas->datasets==2.17.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n",
            "Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.5.0\n",
            "    Uninstalling fsspec-2024.5.0:\n",
            "      Successfully uninstalled fsspec-2024.5.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.20.0\n",
            "    Uninstalling datasets-2.20.0:\n",
            "      Successfully uninstalled datasets-2.20.0\n",
            "Successfully installed datasets-2.17.0 fsspec-2023.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U datasets==2.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Sg9dv0QIWWR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['instruction', 'input', 'output', 'text'],\n",
            "        num_rows: 49620\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 데이터셋 로드\n",
        "dataset = load_dataset(\"royboy0416/ko-alpaca\")\n",
        "\n",
        "# 데이터셋의 구조 확인\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o1basYqSJFys"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': '건강을 유지하기 위한 세 가지 팁을 알려주세요.',\n",
              " 'input': '',\n",
              " 'output': '세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.',\n",
              " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n\\n### Instruction:\\n건강을 유지하기 위한 세 가지 팁을 알려주세요.\\n\\n### Response:\\n세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaQbtRTTovBm"
      },
      "source": [
        "# Gemma 데이터셋 포맷팅\n",
        "\n",
        "```<start_of_turn>user```<br>\n",
        "```What is Cramer's Rule?<end_of_turn>```<br>\n",
        "```<start_of_turn>model```<br>\n",
        "```Cramer's Rule is ...<end_of_turn>```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mMh65FNUKIir"
      },
      "outputs": [],
      "source": [
        "# 'prompt' 필드 생성 함수\n",
        "def format_instruction(example):\n",
        "\n",
        "    # 추가 컨텍스트(input 필드)가 있는 경우\n",
        "    if example['input'] and len(example['input']) > 0:\n",
        "        text = f\"\"\"<start_of_turn>user\\n{example[\"instruction\"]}\\n{example[\"input\"]}<end_of_turn>\\n<start_of_turn>model\\n{example[\"output\"]}<end_of_turn>\"\"\"\n",
        "    # input 필드가 없는 경우\n",
        "    else:\n",
        "        text = f\"\"\"<start_of_turn>user\\n{example[\"instruction\"]}<end_of_turn>\\n<start_of_turn>model\\n{example[\"output\"]}<end_of_turn>\"\"\"\n",
        "\n",
        "    return {'prompt': text}\n",
        "\n",
        "# 데이터셋의 prompt 필드를 업데이트\n",
        "dataset = dataset.map(format_instruction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fs2nTvjMteR9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': '건강을 유지하기 위한 세 가지 팁을 알려주세요.',\n",
              " 'input': '',\n",
              " 'output': '세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.',\n",
              " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n\\n### Instruction:\\n건강을 유지하기 위한 세 가지 팁을 알려주세요.\\n\\n### Response:\\n세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.',\n",
              " 'prompt': '<start_of_turn>user\\n건강을 유지하기 위한 세 가지 팁을 알려주세요.<end_of_turn>\\n<start_of_turn>model\\n세 가지 팁은 아침식사를 꼭 챙기며, 충분한 수면을 취하고, 적극적으로 운동을 하는 것입니다.<end_of_turn>'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YOh_8dkHYPMr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': '홀수 중 하나를 밝히세요.',\n",
              " 'input': '트위터, 인스타그램, 텔레그램',\n",
              " 'output': '텔레그램입니다.',\n",
              " 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n\\n### Instruction:\\n홀수 중 하나를 밝히세요.\\n\\n### Input:\\n트위터, 인스타그램, 텔레그램\\n\\n### Response:\\n텔레그램입니다.',\n",
              " 'prompt': '<start_of_turn>user\\n홀수 중 하나를 밝히세요.\\n트위터, 인스타그램, 텔레그램<end_of_turn>\\n<start_of_turn>model\\n텔레그램입니다.<end_of_turn>'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtExQUBlEFMv"
      },
      "source": [
        "### 모델 로드 및 튜닝:\n",
        "\n",
        "1. 모델 학습: gemma-2b 모델을 로드하고, 준비된 데이터셋을 사용하여 모델을 세부 튜닝합니다. 이 과정에서는 학습률, 에폭 수 등의 파라미터를 조정할 수 있습니다.\n",
        "1. 평가 및 반복: 튜닝된 모델을 평가하고 결과를 확인합니다. 필요에 따라 여러 번 반복하여 모델의 성능을 최적화할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XrefsFXqFM9V"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers==4.38.0 accelerate==0.27.1 bitsandbytes==0.42.0 peft==0.8.2 trl==0.7.10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: accelerate in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from accelerate) (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
            "Requirement already satisfied: requests in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (0.42.0)\n",
            "Requirement already satisfied: scipy in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate\n",
        "!pip install bitsandbytes --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rd1Ya28FujGg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jun 15 21:51:45 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 555.85                 Driver Version: 555.85         CUDA Version: 12.5     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3090      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
            "| 50%   35C    P8             11W /  210W |    1564MiB /  24576MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A    294344      C   ...rograms\\Python\\Python311\\python.exe      N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jRjOEOq9JBju"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import json\n",
        "import time\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from huggingface_hub import notebook_login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9Cn_f9eewOs0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f98d844dfd9472597d8a9e00c3e9bc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 자기 자신의 허깅페이스 토큰 필요\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bZ87WNIPJZLN"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/gemma-2b\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_quant_type=\"nf4\",\n",
        "                                bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "#                                              quantization_config=bnb_config,\n",
        "#                                              device_map={\"\":0})\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/leeshinhaeng/opt/anaconda3/envs/llm24/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m/Users/leeshinhaeng/Desktop/Code/llm_code/1일/gemma_finetuning_koalpaca.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/leeshinhaeng/Desktop/Code/llm_code/1%EC%9D%BC/gemma_finetuning_koalpaca.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_id,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leeshinhaeng/Desktop/Code/llm_code/1%EC%9D%BC/gemma_finetuning_koalpaca.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                              quantization_config\u001b[39m=\u001b[39;49mbnb_config,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leeshinhaeng/Desktop/Code/llm_code/1%EC%9D%BC/gemma_finetuning_koalpaca.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                              device_map\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m0\u001b[39;49m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/leeshinhaeng/Desktop/Code/llm_code/1%EC%9D%BC/gemma_finetuning_koalpaca.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_id, add_eos_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/llm24/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/llm24/lib/python3.10/site-packages/transformers/modeling_utils.py:3024\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m     hf_quantizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3023\u001b[0m \u001b[39mif\u001b[39;00m hf_quantizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m     hf_quantizer\u001b[39m.\u001b[39;49mvalidate_environment(\n\u001b[1;32m   3025\u001b[0m         torch_dtype\u001b[39m=\u001b[39;49mtorch_dtype, from_tf\u001b[39m=\u001b[39;49mfrom_tf, from_flax\u001b[39m=\u001b[39;49mfrom_flax, device_map\u001b[39m=\u001b[39;49mdevice_map\n\u001b[1;32m   3026\u001b[0m     )\n\u001b[1;32m   3027\u001b[0m     torch_dtype \u001b[39m=\u001b[39m hf_quantizer\u001b[39m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3028\u001b[0m     device_map \u001b[39m=\u001b[39m hf_quantizer\u001b[39m.\u001b[39mupdate_device_map(device_map)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/llm24/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:62\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_environment\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_accelerate_available() \u001b[39mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m---> 62\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mand the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         )\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfrom_tf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mor\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfrom_flax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m sure the weights are in PyTorch format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             device_map={\"\":0})\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bN96zjNf7O25"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SPJn3dPwwBll"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e29e8a988a99474ab3f75e91d7de8f6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/49620 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'text', 'prompt', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 39696\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['instruction', 'input', 'output', 'text', 'prompt', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 9924\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NRAQtIgVxfG6"
      },
      "outputs": [],
      "source": [
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FJLdkGrnxmXQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'instruction': '이것은 개방형 생성 과제입니다. GPT 모델은 명령에 적합한 출력을 생성해야 합니다.', 'input': '면접을 준비하는 방법', 'output': '면접을 준비하기 위해 관련된 질문 목록을 작성하고, 효과적인 답변 전략을 개발하여 자신감 있게 나설 수 있도록 해보세요.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request. \\n\\n### Instruction:\\n이것은 개방형 생성 과제입니다. GPT 모델은 명령에 적합한 출력을 생성해야 합니다.\\n\\n### Input:\\n면접을 준비하는 방법\\n\\n### Response:\\n면접을 준비하기 위해 관련된 질문 목록을 작성하고, 효과적인 답변 전략을 개발하여 자신감 있게 나설 수 있도록 해보세요.', 'prompt': '<start_of_turn>user\\n이것은 개방형 생성 과제입니다. GPT 모델은 명령에 적합한 출력을 생성해야 합니다.\\n면접을 준비하는 방법<end_of_turn>\\n<start_of_turn>model\\n면접을 준비하기 위해 관련된 질문 목록을 작성하고, 효과적인 답변 전략을 개발하여 자신감 있게 나설 수 있도록 해보세요.<end_of_turn>', 'input_ids': [2, 106, 1645, 108, 235832, 139988, 49532, 238037, 238867, 116518, 65084, 236939, 47555, 235265, 162174, 162570, 236648, 95165, 240446, 236179, 99797, 237961, 236511, 182260, 236392, 116518, 149735, 179694, 235265, 108, 237722, 240449, 236392, 166422, 237584, 40284, 130059, 107, 108, 106, 2516, 108, 237722, 240449, 236392, 166422, 237584, 72159, 93806, 187003, 238602, 160587, 237465, 154124, 236392, 198766, 48060, 235269, 207092, 237233, 85024, 235248, 241305, 239042, 31087, 242296, 236392, 227613, 72494, 127637, 239199, 21167, 237458, 38585, 238551, 22618, 21167, 153145, 56787, 237036, 96673, 235265, 107, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pO4pjPKdxny-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mkh08\\OneDrive\\문서\\GitHub\\Ollama_TestCode\\venv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:555: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user\n",
            "  건강을 유지하기 위한 세 가지 팁을 알려주세요.\n",
            "  \n",
            "  model\n",
            "  \n",
            "  abbaye\n",
            "  이것은 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 궁극적으로 \n"
          ]
        }
      ],
      "source": [
        "def get_completion(query: str, model, tokenizer):\n",
        "\n",
        "  prompt_template = \"\"\"<start_of_turn>user\n",
        "  {query}\n",
        "  <end_of_turn>\n",
        "  <start_of_turn>model\n",
        "  \"\"\"\n",
        "  prompt = prompt_template.format(query=query)\n",
        "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "  model_inputs = encodeds.to(\"cuda:0\")\n",
        "  generated_ids = model.generate(**model_inputs, max_new_tokens=256)\n",
        "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "  return decoded\n",
        "\n",
        "# Fine tuning 이전\n",
        "result = get_completion(query=\"건강을 유지하기 위한 세 가지 팁을 알려주세요.\", model=model, tokenizer=tokenizer)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NTmSRl0Fz4KY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mkh08\\OneDrive\\문서\\GitHub\\Ollama_TestCode\\venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef624ac9dd7a4ad5bf19ea8df03242da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/39696 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49ecdd7846ea4f2790caadc14bbe3f2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/9924 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mkh08\\OneDrive\\문서\\GitHub\\Ollama_TestCode\\venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc40818a3de04b3ba3e983c441201c84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.7332, 'grad_norm': 0.8657228350639343, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
            "{'loss': 3.0952, 'grad_norm': 1.0215117931365967, 'learning_rate': 0.00017777777777777779, 'epoch': 0.0}\n",
            "{'loss': 2.6608, 'grad_norm': 1.099008560180664, 'learning_rate': 0.00015555555555555556, 'epoch': 0.0}\n",
            "{'loss': 2.3759, 'grad_norm': 0.8393830060958862, 'learning_rate': 0.00013333333333333334, 'epoch': 0.0}\n",
            "{'loss': 2.2601, 'grad_norm': 1.1383107900619507, 'learning_rate': 0.00011111111111111112, 'epoch': 0.01}\n",
            "{'loss': 2.3429, 'grad_norm': 0.7046269178390503, 'learning_rate': 8.888888888888889e-05, 'epoch': 0.01}\n",
            "{'loss': 2.3607, 'grad_norm': 2.020468235015869, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}\n",
            "{'loss': 2.1884, 'grad_norm': 0.699824869632721, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.01}\n",
            "{'loss': 2.1641, 'grad_norm': 0.8179895281791687, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.01}\n",
            "{'loss': 2.2895, 'grad_norm': 0.5826743841171265, 'learning_rate': 0.0, 'epoch': 0.01}\n",
            "{'train_runtime': 56.9655, 'train_samples_per_second': 7.022, 'train_steps_per_second': 1.755, 'train_loss': 2.547081050872803, 'epoch': 0.01}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=2.547081050872803, metrics={'train_runtime': 56.9655, 'train_samples_per_second': 7.022, 'train_steps_per_second': 1.755, 'train_loss': 2.547081050872803, 'epoch': 0.01})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    target_modules=['o_proj', 'q_proj', 'up_proj', 'v_proj', 'k_proj', 'down_proj', 'gate_proj'],\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    dataset_text_field=\"prompt\",\n",
        "    peft_config=lora_config,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "    ),\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OeygbIZg24Y_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user\n",
            "  건강을 유지하기 위한 세 가지 팁을 알려주세요.\n",
            "  \n",
            "  model\n",
            "   1. 규칙적인 운동을 하세요.\n",
            "  2. 건강한 식사를 즐기세요.\n",
            "  3. 규칙적인 수면을 취하세요.\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur: 3\n",
            "   Vainqueur:\n"
          ]
        }
      ],
      "source": [
        "# Fine tuning 이후\n",
        "result = get_completion(query=\"건강을 유지하기 위한 세 가지 팁을 알려주세요.\",\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "09pIPcLpBuAP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user\n",
            "  불면증을 해결하는 방법을 세 가지 알려주세요.\n",
            "  \n",
            "  model\n",
            "   1. 불면증을 해결하는 방법을 세 가지 알려주세요.\n",
            "  2. 불면증을 해결하는 방법을 세 가지 알려주세요.\n",
            "  3. 불면증을 해결하는 방법을 세 가지 알려주세요.\n",
            "  :+::+: 델타:+:\n",
            "  :+:+:+:+: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+::::: 델타:+:\n",
            "  :+:::::\n"
          ]
        }
      ],
      "source": [
        "# Fine tuning 이후\n",
        "result = get_completion(query=\"불면증을 해결하는 방법을 세 가지 알려주세요.\",\n",
        "                        model=trainer.model,\n",
        "                        tokenizer=tokenizer)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH9FFSWS2uue"
      },
      "source": [
        "# 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lLGaVpqY2n7a"
      },
      "outputs": [],
      "source": [
        "new_model = \"gemma-2b-it-koalpaca-finetuned\"\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
