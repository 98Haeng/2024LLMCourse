{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama run test : ollama pull qwen2:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 저는 유사하게 인간이 사용할 수 있는 인터페이스를 제공하는 AI 어시스턴트입니다. 어떤 도움을 드릴까요?\n"
     ]
    }
   ],
   "source": [
    "#local ollama inference\n",
    "\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='qwen2:1.5b', messages=[\n",
    "    {'role': 'system', 'content': '당신은 유용한 인공지능 어시스턴트입니다.'},\n",
    "    {'role': 'user', 'content': '안녕하세요. 자기소개 부탁합니다.'}\n",
    "])\n",
    "\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrok 터널링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "#base_url은 잘 바뀌니, ngrok으로 터널링할 때마다 잘 바꿔줄 것 \n",
    "model = ChatOllama(model=\"qwen2:1.5b\", temperature=0,\n",
    "                   base_url=\"https://d334-34-83-101-157.ngrok-free.app/\") \n",
    "\n",
    "response = model.invoke(\"안녕하세요. 자기소개를 부탁합니다. \")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능의 지도학습은 인공지능 모델을 학습시키는 과정입니다. 이 과정에서는 데이터를 사용하여 모델에 학습된 정보를 추가하고, 이를 통해 모델이 새로운 데이터에 대해 더 잘 예측할 수 있도록 합니다.\n",
      "\n",
      "인공지능의 지도학습은 두 가지 주요한 방법으로 이루어집니다: 트레이닝과 테스트. 트레이닝 과정에서는 모델을 학습시키는 데 필요한 정보를 제공합니다. 이 정보는 데이터에서 추출되며, 이를 통해 모델이 새로운 데이터에 대해 예측할 수 있게 됩니다.\n",
      "\n",
      "테스트 과정에서는 모델의 정확성을 검사하는 데 사용됩니다. 이 과정에서는 모델이 학습된 정보와 실제 데이터를 비교하여 결과를 확인합니다. \n",
      "\n",
      "인공지능의 지도학습은 데이터의 양과 질에 따라 성능이 달라집니다. 더 많은 데이터가 있고, 데이터가 풍부하고 정확한 경우, 모델의 예측력이 높아집니다.\n"
     ]
    }
   ],
   "source": [
    "#local ollama inference\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "result = llm.invoke(\"인공지능의 지도학습이란?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'비지도 학습은 학습을 통해 새로운 정보를 배우는 과정에서, 학습자가 그 정보를 직접적으로 이해하고 적용하는 것을 의미합니다. 이는 일반적인 교육 시스템과 비슷한 방식으로 이루어집니다.\\n\\n예를 들어, 학생이 새로운 개념을 배우기 위해 문제를 풀고 그 결과를 분석하면, 그 학습자가 그 정보를 이해하고 적용하는 과정을 통해 새로운 정보를 학습합니다. 이는 비지도 학습과 동일한 효과를 가져옵니다.\\n\\n비지도 학습은 교육 시스템에서 사용되는 주요 기법 중 하나로, 학생들이 스스로 학습하고 문제 해결 능력을 향상시키기 위해 사용됩니다.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "result = chain.invoke({\"input\": \"비지도 학습이 무엇인가요?\"})\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP)는 인간이 말하는 언어를 이해하고, 이를 기반으로 정보를 추출하거나 생성하는 프로그래밍 기법입니다. 이 기술은 컴퓨터가 사용자로부터 입력을 받아서 그에 대한 답변을 제공하거나, 사용자가 질문을 하면 그에 대한 정확한 정보를 제공하는 데 사용됩니다.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "chain1.invoke({\"input\": \"자연어 처리란 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural Language Processing (NLP) is a programming methodology that allows humans to understand and extract information from the language they speak. This technology designs computers to interpret human language correctly. Generally, NLP includes Natural Language Processing (NLP), Artificial Intelligence (AI), and Machine Learning (ML) in various fields, enabling computers to effectively process the information humans communicate.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "#chain2\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"당신은 번역가입니다. 다음에 주어진 문장을 영어로 번역해주세요. {text}\"\"\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = {\"text\":chain1} | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": \"자연어 처리란 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Natural Language Processing (NLP) ist ein Programmiertechnik, der menschliche Sprache verstehen und aufgrund dieser Informationen Informationen extrahieren oder erstellen kann. Diese Technologie wird Computer dazu gesteuert, menschliches Worten zu verstehen und reagieren. Normalerweise wird NLP in den Bereichen von Natur- und Künstliche Intelligenz (NLP), Maschinelles Lernen (Machine Learning) und anderen Bereichen verwendet, um Computers zur effizienten Verarbeitung menschlicher Informationen zu helfen.\"'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2\n",
    "lang = {\"lang\" : input(\"언어를 선택해주세요.\")}\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요. {text}\"\"\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": \"자연어 처리란 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'죄송합니다, 제가 인공지능이라 개인적인 경험을 가지고 있지 않아요.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요. {text}\"\"\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"원본: 죄송합니다, 저는 실시간 정보를 제공하지 못합니다. 현재 위치나 시간을 입력해주시면 도움이 될 것 같습니다.\\n번역 결과: Sorry, I can't provide real-time information. If you input your current location and time, it may be helpful.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 다음의 질문에 답해주세요. : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_template(\"\"\"#Instruction \n",
    "                                           당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                           \n",
    "                                           결과 출력 시, 주어진 번역 결과와 원본 문장을 같이 출력해주세요.       \n",
    "               \n",
    "                                           #Result\n",
    "                                        원본 : \n",
    "                                        번역 결과 : {text}                                 \n",
    "                                           \"\"\")\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are Helpful AI BOT. Your name is 명지봇'),\n",
       " HumanMessage(content='이름이 뭐니?')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are Helpful AI BOT. Your name is 명지봇\"),\n",
    "    (\"user\", \"{user_input}\"),\n",
    "])\n",
    "output_parser = StrOutputParser()\n",
    "messages = chat_prompt.format_messages(user_input=\"이름이 뭐니?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나는 명지봇입니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = chat_prompt | llm | output_parser\n",
    "chain.invoke({\"user_input\": \"네 이름이 뭐니?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatTemplate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful AI bot. Your name is 김철수.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='니 이름이 뭐니?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "\n",
    "template3 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"assistant\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "input_value =    {\n",
    "        \"name\": \"김철수\",\n",
    "        \"user_input\": \"니 이름이 뭐니?\"\n",
    "    }\n",
    "\n",
    "prompt_value = template3.invoke(input_value)\n",
    "output_parser = StrOutputParser()\n",
    "print(prompt_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'저는 김철수입니다. 어떻게 도와드릴까요?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3 = template3 | llm | output_parser\n",
    "response = chain3.invoke(input_value)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제해결 : 앞에서 만든 번역 Chain을 from_messages 메소드를 이용하여 인스트럭션을 주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 날씨는?\n",
      "english\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I can't provide real-time weather information. Please let me know your current location or city and I'll do my best to assist you.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_messages([(\"system\", \"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "                                            (\"user\",\"{input}\")\n",
    "                                            ])\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://127.0.0.1:11434\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_messages([(\"system\", \"\"\"\n",
    "                                            # Instruction \n",
    "                                            당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                            \"\"\"),\n",
    "                                            (\"user\", \"\"\"\n",
    "                                            # Text\n",
    "                                            {text}                                           \n",
    "                                            # Result\"\"\")])\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "print(question)\n",
    "print(lang)\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are Helpful AI BOT. Your name is 명지봇'),\n",
       " HumanMessage(content='자기소개를 부탁드립니다.')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate,  HumanMessagePromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are Helpful AI BOT. Your name is 명지봇\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "messages = chat_prompt.format_messages(user_input=\"자기소개를 부탁드립니다.\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 명지봇입니다. 저는 인공지능으로서 사용자의 질문에 답변하고 정보 제공을 도와드릴 수 있습니다. 어떤 도움이 필요하신가요?'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"You are Helpful AI BOT. Your name is 명지봇\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = chat_prompt | llm | output_parser\n",
    "\n",
    "chain.invoke({\"user_input\":\"자기소개를 부탁해요.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제해결 : Message 템플릿으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마이클 잭슨에 대해 설명해주세요\n",
      "영어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Michael Jackson was an American singer, songwriter, producer, television host, film director and musician. He was born in 1964 and is one of the most famous singers from the 1970s and 80s. His music includes various genres such as hip hop and pop rock.\\n\\nHe also wrote and composed for the musical \"Thriller\" which premiered in 1982, and his musical achievements and influence grew significantly after its release. His music includes songs like \"Beat It\", \"Billie Jean\", \"Thriller\" among others.\\n\\nHe also starred in the musical \"The Moon Song\" and it premiered in 1983. His musical achievements and influence are recognized as one of the most famous singers, and he was also recognized as a film actor.\\n\\nMichael Jackson died in 2009 and many people still respect him for his music\\'s impact and personal issues.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "                                            ])\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", temperature=0, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Instruction \n",
    "                                            당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                            \"\"\"),\n",
    "                                            HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Text\n",
    "                                            {text}                                           \n",
    "                                            # Result\"\"\")])\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "print(question)\n",
    "print(lang)\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능\n",
      "영어\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI refers to the technology that imitates human ability'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "\n",
    "\n",
    "# Chain1\n",
    "llm1 = ChatOllama(model=\"qwen2:1.5b\", temperature=0.3, num_predict=50, stop= [\"\\n\"], base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "                                            ])\n",
    "output_parser = StrOutputParser()\n",
    "chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "params = {\n",
    "    \"temperature\": 0.7,         # 생성된 텍스트의 다양성 조정\n",
    "    \"num_predict\": 10,          # 생성할 최대 토큰 수\n",
    "    \"stop\": [\"\\n\"]              # 정지 시퀀스 설정\n",
    "}\n",
    "\n",
    "# Chain2 / 2 input\n",
    "question = input(\"질문: \")\n",
    "lang = input(\"언어를 선택해주세요.\")\n",
    "llm2 = ChatOllama(model=\"qwen2:1.5b\", **params, base_url=\"http://192.168.0.57:11434/\") #http://127.0.0.1:11434\n",
    "prompt2 = ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Instruction \n",
    "                                            당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                            \"\"\"),\n",
    "                                            HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "                                            # Text\n",
    "                                            {text}                                           \n",
    "                                            # Result\"\"\")])\n",
    "output_parser = StrOutputParser()\n",
    "chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "print(question)\n",
    "print(lang)\n",
    "chain2.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메소드화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI is a programming method utilizing complex systems from computer science to improve the capability of human imagination and technological skills by providing ways to generate, analyze, predict, solve problems, and process vision. AI uses computers to understand and apply what humans depend on in terms of science technology, which helps in the actions and decisions of human beings.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chains(question, lang, model):\n",
    "    # Chain1\n",
    "    llm1 = ChatOllama(model=model, base_url=\"http://192.168.0.57:11434/\")\n",
    "    prompt1 = ChatPromptTemplate.from_messages([\n",
    "                SystemMessagePromptTemplate.from_template(\"당신은 인공지능 전문가입니다. 사용자의 질문에 답해주세요.\"),\n",
    "                HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "                                                ])\n",
    "    output_parser = StrOutputParser()\n",
    "    chain1 = prompt1 | llm1 | output_parser\n",
    "\n",
    "    # Chain2\n",
    "    llm2 = ChatOllama(model=model, base_url=\"http://192.168.0.57:11434/\")\n",
    "    prompt2 = ChatPromptTemplate.from_messages([SystemMessagePromptTemplate.from_template(\"\"\"\n",
    "                                                # Instruction \n",
    "                                                당신은 번역가입니다. 다음에 주어진 문장을 {lang}로 번역해주세요.                                                                                      \n",
    "                                                \"\"\"),\n",
    "                                                HumanMessagePromptTemplate.from_template(\"\"\"\n",
    "                                                # Text\n",
    "                                                {text}                                           \n",
    "                                                # Result\"\"\")])\n",
    "    chain2 = RunnableMap({\"text\": RunnableLambda(lambda x: chain1.invoke({\"input\": question})), \"lang\": RunnableLambda(lambda x: lang)}) | prompt2 | llm2 | output_parser\n",
    "    \n",
    "    # Run chain2\n",
    "    result = chain2.invoke({\"input\": question})\n",
    "    return result\n",
    "\n",
    "run_chains(\"인공지능이란?\", \"영어\", \"qwen2:1.5b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
